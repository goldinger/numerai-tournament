{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pandas as pd\n",
    "import numerapi\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from utils import (\n",
    "    # save_model,\n",
    "    # load_model,\n",
    "    neutralize,\n",
    "    get_biggest_change_features,\n",
    "    validation_metrics,\n",
    "    ERA_COL,\n",
    "    DATA_TYPE_COL,\n",
    "    TARGET_COL,\n",
    "    EXAMPLE_PREDS_COL\n",
    ")\n",
    "import json\n",
    "from lightgbm import LGBMRegressor\n",
    "import gc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_id = os.environ.get(\"NUMERAI_PUBLIC_KEY\")\n",
    "secret_key = os.environ.get(\"NUMERAI_SECRET_KEY\")\n",
    "napi = numerapi.NumerAPI(public_id, secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_round = napi.get_current_round()\n",
    "TRAINING_DATA_FILE = \"data/training_data.parquet\"\n",
    "TOURNAMENT_DATA_FILE = f\"data/tournament_data_{current_round}.parquet\"\n",
    "VALIDATION_DATA_FILE = \"data/validation_data.parquet\"\n",
    "EXAMPLE_VALIDATION_PREDICTIONS_FILE = \"data/example_validation_predictions.parquet\"\n",
    "FEATURES_FILE = \"data/features.json\"\n",
    "\n",
    "MODEL_NAME = \"target_model\"\n",
    "TARGET_MODEL_FILE = f\"output/{MODEL_NAME}\"\n",
    "VALIDATION_PREDICTIONS_FILE = f\"output/validation_predictions_{current_round}.csv\"\n",
    "TOURNAMENT_PREDICTIONS_FILE = f\"output/tournament_predictions_{current_round}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 09:40:59,263 INFO numerapi.utils: starting download\n",
      "data/training_data.parquet: 1.01GB [04:11, 4.03MB/s]                             \n",
      "2022-03-16 09:45:11,816 INFO numerapi.utils: starting download\n",
      "data/tournament_data_307.parquet: 582MB [02:34, 3.77MB/s]                             \n",
      "2022-03-16 09:47:47,294 INFO numerapi.utils: target file already exists\n",
      "2022-03-16 09:47:47,295 INFO numerapi.utils: download complete\n",
      "2022-03-16 09:47:48,298 INFO numerapi.utils: target file already exists\n",
      "2022-03-16 09:47:48,298 INFO numerapi.utils: download complete\n",
      "2022-03-16 09:47:49,252 INFO numerapi.utils: target file already exists\n",
      "2022-03-16 09:47:49,253 INFO numerapi.utils: download complete\n"
     ]
    }
   ],
   "source": [
    "# Tournament data changes every week so we specify the round in their name. Training\n",
    "# and validation data only change periodically, so no need to download them every time.\n",
    "print('Downloading dataset files...')\n",
    "napi.download_dataset(\"numerai_training_data.parquet\", TRAINING_DATA_FILE)\n",
    "napi.download_dataset(\"numerai_tournament_data.parquet\", TOURNAMENT_DATA_FILE)\n",
    "napi.download_dataset(\"numerai_validation_data.parquet\", VALIDATION_DATA_FILE)\n",
    "napi.download_dataset(\"example_validation_predictions.parquet\", EXAMPLE_VALIDATION_PREDICTIONS_FILE)\n",
    "napi.download_dataset(\"features.json\", FEATURES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading minimal training data\n"
     ]
    }
   ],
   "source": [
    "print('Reading minimal training data')\n",
    "# read the feature metadata and get the \"small\" feature set\n",
    "with open(FEATURES_FILE, \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "features = feature_metadata[\"feature_sets\"][\"small\"]\n",
    "# read in just those features along with era and target columns\n",
    "read_columns = features + [ERA_COL, DATA_TYPE_COL, TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: sometimes when trying to read the downloaded data you get an error about invalid magic parquet bytes...\n",
    "# if so, delete the file and rerun the napi.download_dataset to fix the corrupted file\n",
    "training_data = pd.read_parquet(TRAINING_DATA_FILE, columns=read_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the per era correlation of each feature vs the target\n",
    "all_feature_corrs = training_data.groupby(ERA_COL).apply(\n",
    "    lambda era: era[features].corrwith(era[TARGET_COL])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the riskiest features by comparing their correlation vs\n",
    "# the target in each half of training data; we'll use these later\n",
    "riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\n",
    "# \"garbage collection\" (gc) gets rid of unused data and frees up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"n_estimators\": 2000,\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"max_depth\": 5,\n",
    "            \"num_leaves\": 2 ** 5,\n",
    "            \"colsample_bytree\": 0.1}\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "# train on all of train and save the model so we don't have to train next time\n",
    "# spinner.start('Training model')\n",
    "model.fit(training_data.filter(like='feature_', axis='columns'),\n",
    "            training_data[TARGET_COL])\n",
    "# print(f\"saving new model: {TARGET_MODEL_FILE}\")\n",
    "# save_model(model, TARGET_MODEL_FILE)\n",
    "# spinner.succeed()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nans per column this week: target_nomi_20    5307\n",
      "dtype: int64\n",
      "out of 5307 total rows\n",
      "filling nans with 0.5\n"
     ]
    }
   ],
   "source": [
    "validation_data = pd.read_parquet(VALIDATION_DATA_FILE, columns=read_columns)\n",
    "tournament_data = pd.read_parquet(TOURNAMENT_DATA_FILE, columns=read_columns)\n",
    "nans_per_col = tournament_data[tournament_data[\"data_type\"] == \"live\"].isna().sum()\n",
    "\n",
    "# check for nans and fill nans\n",
    "if nans_per_col.any():\n",
    "    total_rows = len(tournament_data[tournament_data[\"data_type\"] == \"live\"])\n",
    "    print(f\"Number of nans per column this week: {nans_per_col[nans_per_col > 0]}\")\n",
    "    print(f\"out of {total_rows} total rows\")\n",
    "    print(f\"filling nans with 0.5\")\n",
    "    tournament_data.loc[:, features] = tournament_data.loc[:, features].fillna(0.5)\n",
    "else:\n",
    "    print(\"No nans in the features this week!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check the feature that the model expects vs what is available to prevent our\n",
    "# pipeline from failing if Numerai adds more data and we don't have time to retrain!\n",
    "model_expected_features = model.booster_.feature_name()\n",
    "if set(model_expected_features) != set(features):\n",
    "    print(f\"New features are available! Might want to retrain model {MODEL_NAME}.\")\n",
    "validation_data.loc[:, f\"preds_{MODEL_NAME}\"] = model.predict(\n",
    "    validation_data.loc[:, model_expected_features])\n",
    "tournament_data.loc[:, f\"preds_{MODEL_NAME}\"] = model.predict(\n",
    "    tournament_data.loc[:, model_expected_features])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutralize our predictions to the riskiest features\n",
    "validation_data[f\"preds_{MODEL_NAME}_neutral_riskiest_50\"] = neutralize(\n",
    "    df=validation_data,\n",
    "    columns=[f\"preds_{MODEL_NAME}\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")\n",
    "\n",
    "tournament_data[f\"preds_{MODEL_NAME}_neutral_riskiest_50\"] = neutralize(\n",
    "    df=tournament_data,\n",
    "    columns=[f\"preds_{MODEL_NAME}\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_to_submit = f\"preds_{MODEL_NAME}_neutral_riskiest_50\"\n",
    "\n",
    "# rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to csv\n",
    "validation_data[\"prediction\"].to_csv(VALIDATION_PREDICTIONS_FILE)\n",
    "tournament_data[\"prediction\"].to_csv(TOURNAMENT_PREDICTIONS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_preds = pd.read_parquet(EXAMPLE_VALIDATION_PREDICTIONS_FILE)\n",
    "validation_data[EXAMPLE_PREDS_COL] = validation_preds[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                        |     mean |   sharpe |\n",
      "|:---------------------------------------|---------:|---------:|\n",
      "| preds_target_model_neutral_riskiest_50 | 0.021721 |  1.16183 |\n"
     ]
    }
   ],
   "source": [
    "# get some stats about each of our models to compare...\n",
    "# fast_mode=True so that we skip some of the stats that are slower to calculate\n",
    "validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4f9bb9be7857ddc1c773a2e4a9cd23aafe0750b569e0870abf8bddbbd312317"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
