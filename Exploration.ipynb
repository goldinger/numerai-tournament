{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pandas as pd\n",
    "import numerapi\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from utils import (\n",
    "    validation_metrics,\n",
    "    EXAMPLE_PREDS_COL\n",
    ")\n",
    "import numpy as np\n",
    "import scipy\n",
    "import utils\n",
    "import json\n",
    "from lightgbm import LGBMRegressor\n",
    "import gc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_id = os.environ.get(\"NUMERAI_PUBLIC_KEY\")\n",
    "secret_key = os.environ.get(\"NUMERAI_SECRET_KEY\")\n",
    "napi = numerapi.NumerAPI(public_id, secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_round = napi.get_current_round()\n",
    "TRAINING_DATA_FILE = \"data/training_data.parquet\"\n",
    "TOURNAMENT_DATA_FILE = f\"data/tournament_data_{current_round}.parquet\"\n",
    "VALIDATION_DATA_FILE = \"data/validation_data.parquet\"\n",
    "EXAMPLE_VALIDATION_PREDICTIONS_FILE = \"data/example_validation_predictions.parquet\"\n",
    "FEATURES_FILE = \"data/features.json\"\n",
    "\n",
    "MODEL_NAME = \"target_model\"\n",
    "TARGET_MODEL_FILE = f\"output/{MODEL_NAME}\"\n",
    "VALIDATION_PREDICTIONS_FILE = f\"output/validation_predictions_{current_round}.csv\"\n",
    "TOURNAMENT_PREDICTIONS_FILE = f\"output/tournament_predictions_{current_round}.csv\"\n",
    "\n",
    "TARGET_COL = 'target'\n",
    "ERA_COL = 'era'\n",
    "DATA_TYPE_COL = 'data_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 11:58:14,365 INFO numerapi.utils: target file already exists\n",
      "2022-03-17 11:58:14,365 INFO numerapi.utils: download complete\n",
      "2022-03-17 11:58:15,612 INFO numerapi.utils: target file already exists\n",
      "2022-03-17 11:58:15,614 INFO numerapi.utils: download complete\n",
      "2022-03-17 11:58:28,996 INFO numerapi.utils: target file already exists\n",
      "2022-03-17 11:58:28,997 INFO numerapi.utils: download complete\n",
      "2022-03-17 11:58:42,312 INFO numerapi.utils: target file already exists\n",
      "2022-03-17 11:58:42,313 INFO numerapi.utils: download complete\n",
      "2022-03-17 11:58:46,794 INFO numerapi.utils: target file already exists\n",
      "2022-03-17 11:58:46,796 INFO numerapi.utils: download complete\n"
     ]
    }
   ],
   "source": [
    "# Tournament data changes every week so we specify the round in their name. Training\n",
    "# and validation data only change periodically, so no need to download them every time.\n",
    "print('Downloading dataset files...')\n",
    "napi.download_dataset(\"numerai_training_data.parquet\", TRAINING_DATA_FILE)\n",
    "napi.download_dataset(\"numerai_tournament_data.parquet\", TOURNAMENT_DATA_FILE)\n",
    "napi.download_dataset(\"numerai_validation_data.parquet\", VALIDATION_DATA_FILE)\n",
    "napi.download_dataset(\"example_validation_predictions.parquet\", EXAMPLE_VALIDATION_PREDICTIONS_FILE)\n",
    "napi.download_dataset(\"features.json\", FEATURES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>targets</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stats</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nb_cols</th>\n",
       "      <td>1050</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         features  targets  other\n",
       "stats                            \n",
       "nb_cols      1050       21      2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns = utils.get_all_columns(TRAINING_DATA_FILE)\n",
    "features = [c for c in all_columns if c.startswith(\"feature_\")]\n",
    "targets = [c for c in all_columns if c.startswith(\"target_\")]\n",
    "other_columns = [c for c in all_columns if not c.startswith(\"feature_\") and not c.startswith(\"target_\")]\n",
    "pd.DataFrame([{\n",
    "    \"features\": len(features),\n",
    "    \"targets\": len(targets),\n",
    "    \"other\": len(other_columns),\n",
    "    \"stats\": \"nb_cols\"\n",
    "}]).set_index(\"stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading minimal training data\n"
     ]
    }
   ],
   "source": [
    "print('Reading minimal training data')\n",
    "# read the feature metadata and get the \"small\" feature set\n",
    "with open(FEATURES_FILE, \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "features = feature_metadata[\"feature_sets\"][\"small\"]\n",
    "# read in just those features along with era and target columns\n",
    "read_columns = features + [ERA_COL, DATA_TYPE_COL] + targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: sometimes when trying to read the downloaded data you get an error about invalid magic parquet bytes...\n",
    "# if so, delete the file and rerun the napi.download_dataset to fix the corrupted file\n",
    "training_data = pd.read_parquet(TRAINING_DATA_FILE, columns=read_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the per era correlation of each feature vs the target\n",
    "all_feature_corrs = training_data.groupby(ERA_COL).apply(\n",
    "    lambda era: era[features].corrwith(era[TARGET_COL])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the riskiest features by comparing their correlation vs\n",
    "# the target in each half of training data; we'll use these later\n",
    "def get_biggest_change_features(corrs, n=None):\n",
    "    all_eras = corrs.index.sort_values()\n",
    "    if n is None:\n",
    "        n = len(corrs.columns) // 2\n",
    "    h1_eras = all_eras[:len(all_eras) // 2]\n",
    "    h2_eras = all_eras[len(all_eras) // 2:]\n",
    "\n",
    "    h1_corr_means = corrs.loc[h1_eras, :].mean()\n",
    "    h2_corr_means = corrs.loc[h2_eras, :].mean()\n",
    "\n",
    "    corr_diffs = h2_corr_means - h1_corr_means\n",
    "    worst_n = corr_diffs.abs().sort_values(ascending=False).head(n).index.tolist()\n",
    "    return worst_n\n",
    "\n",
    "riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\n",
    "# \"garbage collection\" (gc) gets rid of unused data and frees up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"n_estimators\": 2000,\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"max_depth\": 5,\n",
    "            \"num_leaves\": 2 ** 5,\n",
    "            \"colsample_bytree\": 0.1}\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "# train on all of train and save the model so we don't have to train next time\n",
    "# spinner.start('Training model')\n",
    "model.fit(training_data.filter(like='feature_', axis='columns'),\n",
    "            training_data[TARGET_COL])\n",
    "# print(f\"saving new model: {TARGET_MODEL_FILE}\")\n",
    "# save_model(model, TARGET_MODEL_FILE)\n",
    "# spinner.succeed()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nans in the features this week!\n"
     ]
    }
   ],
   "source": [
    "validation_data = pd.read_parquet(VALIDATION_DATA_FILE, columns=read_columns)\n",
    "tournament_data = pd.read_parquet(TOURNAMENT_DATA_FILE, columns=read_columns)\n",
    "\n",
    "tournament_data_features_only = tournament_data[features + [DATA_TYPE_COL]]\n",
    "nans_per_col = tournament_data_features_only[tournament_data_features_only[\"data_type\"] == \"live\"].isna().sum()\n",
    "del tournament_data_features_only\n",
    "# check for nans and fill nans\n",
    "if nans_per_col.any():\n",
    "    total_rows = len(tournament_data[tournament_data[DATA_TYPE_COL] == \"live\"])\n",
    "    print(f\"Number of nans per column this week: {nans_per_col[nans_per_col > 0]}\")\n",
    "    print(f\"out of {total_rows} total rows\")\n",
    "    print(f\"filling nans with 0.5\")\n",
    "    tournament_data.loc[:, features] = tournament_data.loc[:, features].fillna(0.5)\n",
    "else:\n",
    "    print(\"No nans in the features this week!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check the feature that the model expects vs what is available to prevent our\n",
    "# pipeline from failing if Numerai adds more data and we don't have time to retrain!\n",
    "model_expected_features = model.booster_.feature_name()\n",
    "if set(model_expected_features) != set(features):\n",
    "    print(f\"New features are available! Might want to retrain model {MODEL_NAME}.\")\n",
    "validation_data.loc[:, f\"preds_{MODEL_NAME}\"] = model.predict(\n",
    "    validation_data.loc[:, model_expected_features])\n",
    "tournament_data.loc[:, f\"preds_{MODEL_NAME}\"] = model.predict(\n",
    "    tournament_data.loc[:, model_expected_features])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def neutralize(df,\n",
    "               columns,\n",
    "               neutralizers=None,\n",
    "               proportion=1.0,\n",
    "               normalize=True,\n",
    "               era_col=\"era\"):\n",
    "    if neutralizers is None:\n",
    "        neutralizers = []\n",
    "    unique_eras = df[era_col].unique()\n",
    "    computed = []\n",
    "    for u in unique_eras:\n",
    "        df_era = df[df[era_col] == u]\n",
    "        scores = df_era[columns].values\n",
    "        if normalize:\n",
    "            scores2 = []\n",
    "            for x in scores.T:\n",
    "                x = (scipy.stats.rankdata(x, method='ordinal') - .5) / len(x)\n",
    "                x = scipy.stats.norm.ppf(x)\n",
    "                scores2.append(x)\n",
    "            scores = np.array(scores2).T\n",
    "        exposures = df_era[neutralizers].values\n",
    "\n",
    "        scores -= proportion * exposures.dot(\n",
    "            np.linalg.pinv(exposures.astype(np.float32)).dot(scores.astype(np.float32)))\n",
    "\n",
    "        scores /= scores.std(ddof=0)\n",
    "\n",
    "        computed.append(scores)\n",
    "\n",
    "    return pd.DataFrame(np.concatenate(computed),\n",
    "                        columns=columns,\n",
    "                        index=df.index)\n",
    "\n",
    "\n",
    "\n",
    "# neutralize our predictions to the riskiest features\n",
    "validation_data[f\"preds_{MODEL_NAME}_neutral_riskiest_50\"] = neutralize(\n",
    "    df=validation_data,\n",
    "    columns=[f\"preds_{MODEL_NAME}\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")\n",
    "\n",
    "tournament_data[f\"preds_{MODEL_NAME}_neutral_riskiest_50\"] = neutralize(\n",
    "    df=tournament_data,\n",
    "    columns=[f\"preds_{MODEL_NAME}\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_to_submit = f\"preds_{MODEL_NAME}_neutral_riskiest_50\"\n",
    "\n",
    "# rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to csv\n",
    "validation_data[\"prediction\"].to_csv(VALIDATION_PREDICTIONS_FILE)\n",
    "tournament_data[\"prediction\"].to_csv(TOURNAMENT_PREDICTIONS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_preds = pd.read_parquet(EXAMPLE_VALIDATION_PREDICTIONS_FILE)\n",
    "validation_data[EXAMPLE_PREDS_COL] = validation_preds[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                        |      mean |   sharpe |\n",
      "|:---------------------------------------|----------:|---------:|\n",
      "| preds_target_model_neutral_riskiest_50 | 0.0205944 | 0.742502 |\n"
     ]
    }
   ],
   "source": [
    "# get some stats about each of our models to compare...\n",
    "# fast_mode=True so that we skip some of the stats that are slower to calculate\n",
    "validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4f9bb9be7857ddc1c773a2e4a9cd23aafe0750b569e0870abf8bddbbd312317"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
